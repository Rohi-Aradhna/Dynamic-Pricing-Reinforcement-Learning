# Dynamic-Pricing-Reinforcement-Learning
Dynamic pricing simulation using reinforcement learning. Models demand with a logistic curve and compares Îµ-Greedy, Thompson Sampling, Greedy, and tuned UCB1. Achieves 55â€“68% optimal-price selection with fast convergence, showing how bandit algorithms adapt to customer behavior.


1. Project Overview

This project builds a dynamic pricing system that tests multiple pricing strategies under uncertainty.
Demand is modeled using a logistic function, and algorithms learn which price maximizes expected revenue over repeated customer interactions.

The simulation evaluates:

Greedy

Îµ-Greedy

Thompson Sampling

UCB1 (tuned version)

ğŸ“ˆ 2. Market Environment

The customer purchase probability is defined by a logistic demand curve:

ğ‘ƒ
(
purchase
)
=
ğ‘
1
+
ğ‘’
ğ‘
â‹…
price
P(purchase)=
1+e
bâ‹…price
a
	â€‹


Where:

a controls purchase scale

b controls price sensitivity

Each algorithm interacts with this environment and receives a binary reward (purchase = 1, no purchase = 0).

3. Algorithms Implemented
Greedy

Always selects the arm with the highest estimated reward.
Fails due to no exploration.

Îµ-Greedy

Explores with probability Îµ (small) and exploits otherwise.
Strong balance â†’ good performance.

Thompson Sampling

Samples from Beta posterior distributions for each arm.
Highly effective and adaptive.

UCB1 (Improved Version)

Uses confidence bounds with:

reward normalization

tunable exploration constant C

After tuning (C â‰ˆ 0.7), UCB1 becomes the best performer.

4. Simulation Setup
Parameter	Value
Price options	[20, 30, 40, 50, 60]
Steps per simulation	10,000
Repeated runs	1,000 epochs
Total interactions	~10 million

For each strategy, the simulation tracks:

Optimal price selection %

Reactivity (how quickly it converges)

Arm allocation distribution

5. Results Summary
1. Baseline Results

Îµ-Greedy and Thompson Sampling performed best initially

Both selected the optimal price in 55%+ of rounds

Both showed fast convergence (~1800â€“1900 steps)

2. After Tuning UCB1

Tuned UCB1 showed:

~68% optimal-price selection

More stable adaptation

Strongest performance across metrics

3. Final Ranking

Tuned UCB1 (C â‰ˆ 0.7)

Îµ-Greedy

Thompson Sampling

Greedy (poor performance)

6. Metrics Used
ğŸ”¹ Arm Allocation

Distribution of how often each algorithm selected each price.

ğŸ”¹ Reactivity

The number of steps required to settle on the best price.

ğŸ”¹ Reward Trend

How revenue or reward changed over time.

(Regret metric exists but not emphasized here.)

Key Takeaways

Demand curves + reinforcement learning = fast, adaptive pricing

Îµ-Greedy and Thompson Sampling are strong baselines

A tuned UCB1 model can outperform both by improving exploration behavior

Simulation-based RL is a powerful tool for real-world pricing strategy
